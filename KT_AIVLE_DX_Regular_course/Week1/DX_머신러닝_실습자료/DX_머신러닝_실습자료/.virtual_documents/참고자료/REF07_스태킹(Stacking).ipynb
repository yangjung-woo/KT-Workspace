








# 라이브러리 불러오기
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import warnings

warnings.filterwarnings(action='ignore')
%config InlineBackend.figure_format = 'retina'


# 데이터 읽어오기
path = 'https://raw.githubusercontent.com/Jangrae/csv/master/diabetes.csv'
data = pd.read_csv(path)





# 상위 몇 개 행 확인
data.head()


# 하위 몇 개 행 확인
data.tail()


# 변수 확인
data.info()








# target 확인
target = 'Outcome'

# 데이터 분리
x = data.drop(columns=target)
y = data.loc[:, target]





# 모듈 불러오기
from sklearn.model_selection import train_test_split

# 7:3으로 분리
x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.3, random_state=1) 








# 불러오기
from sklearn.tree import DecisionTreeClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
from sklearn.ensemble import StackingClassifier
from sklearn.model_selection import cross_val_predict, GridSearchCV
from lightgbm import LGBMClassifier

from sklearn.preprocessing import MinMaxScaler
from sklearn.pipeline import make_pipeline
from sklearn.metrics import *





# KNN
model_knn = make_pipeline(MinMaxScaler(), KNeighborsClassifier())
model_knn.fit(x_train, y_train)

# Decision Tree
model_dt = DecisionTreeClassifier(random_state=1)
model_dt.fit(x_train, y_train)

# Logistic Regression
model_lr = LogisticRegression()
model_lr.fit(x_train, y_train)

# Light GBM
model_lgb = LGBMClassifier(random_state=1, verbose=-1)
model_lgb.fit(x_train, y_train)





# 예측 결과 수집 (교차검증으로 훈련 , 모델 생성)
pred_dict = {'p1': cross_val_predict(model_lr, x_train, y_train, cv=5),
             'p2': cross_val_predict(model_knn, x_train, y_train, cv=5),
             'p3': cross_val_predict(model_dt, x_train, y_train, cv=5),
             'p4': cross_val_predict(model_lgb, x_train, y_train, cv=5)}

# 데이터프레임 선언
result = pd.DataFrame(pred_dict)
result


# 최종 모델 학습 렌덤 포레스트 생성
final_model = RandomForestClassifier(random_state=1)
final_model.fit(result, y_train)





# 예측 결과 수집 (검증용 데이터로 실제 예측값 확인 )
pred_dict = {'p1': model_lr.fit(x_train, y_train).predict(x_test),
             'p2': model_knn.fit(x_train, y_train).predict(x_test),
             'p3': model_dt.fit(x_train, y_train).predict(x_test),
             'p4': model_lgb.fit(x_train, y_train).predict(x_test)}

# 데이터프레임 선언
result = pd.DataFrame(pred_dict)

# 최종 모델 예측
y_pred = final_model.predict(result)

# 평가하기
print(confusion_matrix(y_test, y_pred))
print(classification_report(y_test, y_pred))





# 스태킹 모델 선언
estimators = [('lr', model_lr), 
              ('dt', model_dt), 
              ('knn', model_knn), 
              ('lgb', model_lgb)]

model = StackingClassifier(estimators=estimators, final_estimator=RandomForestClassifier(random_state=1))

# 학습하기
model.fit(x_train, y_train)

# 예측하기
y_pred = model.predict(x_test)

# 평가하기
print(confusion_matrix(y_test, y_pred))
print(classification_report(y_test, y_pred))





# 개별 모델 성능 확인
for m in [model_knn, model_dt, model_lr, model_lgb]:
    m.fit(x_train, y_train)
    y_pred = m.predict(x_test)
    print(f'* {m.__class__.__name__} Accuracy Score: {accuracy_score(y_test, y_pred):.4f}')








# KNN
param = {'kneighborsclassifier__n_neighbors': range(2, 11, 1)}
model_grid = GridSearchCV(make_pipeline(MinMaxScaler(), KNeighborsClassifier()), param)
model_grid.fit(x_train, y_train)
model_knn = model_grid.best_estimator_

# Decision Tree
param = {'max_depth': range(1, 11, 1)}
model_grid = GridSearchCV(DecisionTreeClassifier(), param)
model_grid.fit(x_train, y_train)
model_dt = model_grid.best_estimator_

# Logistic Regression
model_lr = LogisticRegression()
model_lr = model_lr.fit(x_train, y_train)

# LightGBM
param = {'max_depth': range(1, 11, 1)}
model_grid = GridSearchCV(LGBMClassifier(verbose=-1), param)
model_grid.fit(x_train, y_train)
model_lgb = model_grid.best_estimator_





# 스태킹 모델 선언
estimators = [('lr', model_lr), 
              ('dt', model_dt), 
              ('knn', model_knn), 
              ('lgb', model_lgb)]

model = StackingClassifier(estimators=estimators, final_estimator=RandomForestClassifier(random_state=1))

# 학습하기
model.fit(x_train, y_train)

# 예측하기
y_pred = model.predict(x_test)

# 평가하기
print(confusion_matrix(y_test, y_pred))
print(classification_report(y_test, y_pred))





# 개별 모델 성능 확인
for m in [model_knn, model_dt, model_lr, model_lgb]:
    y_pred = m.predict(x_test)
    print(f'* {m.__class__.__name__} Accuracy Score: {accuracy_score(y_test, y_pred):.4f}')
