











# 라이브러리 불러오기
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import warnings

warnings.filterwarnings(action='ignore')
%config InlineBackend.figure_format = 'retina'


# 데이터 불러오기
path = 'https://raw.githubusercontent.com/jangrae/csv/master/admission_simple.csv'
data = pd.read_csv(path)





# 데이터 살펴보기
data.head()


# 기술통계 확인
data.describe()








# target 확인
target = 'ADMIT'

# 데이터 분리
x = data.drop(columns=target)
y = data.loc[:, target]





# 모듈 불러오기
from sklearn.model_selection import train_test_split

# 7:3으로 분리
x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.3, random_state=1)





# 모듈 불러오기
from sklearn.preprocessing import MinMaxScaler

# 정규화
scaler = MinMaxScaler()
x_train_s = scaler.fit_transform(x_train)
x_test_s = scaler.transform(x_test)





# xgboost 설치
#!pip install xgboost


# lightgbm 설치
#!pip install lightgbm


# 라이브러리 불러오기
from sklearn.neighbors import KNeighborsClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
from xgboost import XGBClassifier
from lightgbm import LGBMClassifier
from sklearn.metrics import *





# 선언하기
model = KNeighborsClassifier(n_neighbors=5)


# 학습하기
model.fit(x_train_s, y_train)


# 예측하기
y_pred = model.predict(x_test_s)


# 평가하기
print(confusion_matrix(y_test, y_pred))
print(classification_report(y_test, y_pred))





# 선언하기
model = DecisionTreeClassifier(max_depth=5, random_state=1)


# 학습하기
model.fit(x_train, y_train)


# 예측하기
y_pred = model.predict(x_test)


# 5단계: 평가하기
print(confusion_matrix(y_test, y_pred))
print(classification_report(y_test, y_pred))





# 선언하기
model = LogisticRegression()


# 학습하기
model.fit(x_train, y_train)


# 예측하기
y_pred = model.predict(x_test)


# 5단계: 평가하기
print(confusion_matrix(y_test, y_pred))
print(classification_report(y_test, y_pred))





# 선언하기
#from sklearn.ensemble import RandomForestClassifier
model = RandomForestClassifier(max_depth=5 ) #랜덤포레스트에서 반드시 max depth를 신경써줘야 함 


# 학습하기
model.fit(x_train,y_train)


# 예측하기
y_pred = model.predict(x_test)


# 5단계: 평가하기
print(confusion_matrix(y_test,y_pred))
print(classification_report(y_test,y_pred))


# Feature 중요도 확인
plt.barh(x.keys(), model.feature_importances_)
plt.show()






# 선언하기
#from xgboost import XGBClassifier
model = XGBClassifier(max_depth = 5)


# 학습하기
model.fit(x_train,y_train)


# 예측하기
y_pred = model.predict(x_test)


# 평가하기
print(confusion_matrix(y_test,y_pred))
print(classification_report(y_test,y_pred))



# Feature 중요도 확인
plt.barh(x.keys(), model.feature_importances_)
plt.show()





# 선언하기
#from lightgbm import LGBMClassifier
model = LGBMClassifier(max_depth = 5,verbose = -1, importance_type = 'gain') 
# importance_type = 'split' 디폴트(분할하는데 몃번 parameter가 사용되었는지 알려줌)
# importance_type = 'gain' 줄인 불순도 양을 알려줌 


# 학습하기
model.fit(x_train, y_train) # 좀더 복잡하게 모델 생성함  > verbose = -1 하면 학습 과정을 굳이 출력 안해줌


# 예측하기
y_pred = model.predict(x_test)


# 평가하기
print(confusion_matrix(y_test,y_pred))
print(classification_report(y_test,y_pred))


# Feature 중요도 확인
tmp = model.feature_importances_
tmp_norm = tmp / np.sum(tmp)
plt.barh(x.keys(),tmp_norm)
plt.show()






