











# 라이브러리 불러오기
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import warnings

warnings.filterwarnings(action='ignore')
%config InlineBackend.figure_format = 'retina'


# 데이터 불러오기
path = 'https://raw.githubusercontent.com/jangrae/csv/master/mobile_cust_churn.csv'
data = pd.read_csv(path)
data['CHURN'] = data['CHURN'].map({'STAY':0, 'LEAVE': 1}) # light xgb는 문자열 target을 인식하지 못함 








# 데이터 살펴보기
data.head()



# 기술통계 확인
data.describe().T




# 변수 관련 정보 확인
data.info()










# 변수 제거: id
data.drop(columns = 'id' ,inplace=True)

data.head()





# x, y 분리
target = "CHURN"

x = data.drop(columns = target)
y = data.loc[:,target]






# 가변수화
dummy_cols = ['REPORTED_SATISFACTION','CONSIDERING_CHANGE_OF_PLAN','REPORTED_USAGE_LEVEL']

x = pd.get_dummies(data=x , columns = dummy_cols , drop_first = True , dtype = int)

x





# 학습용, 평가용 데이터 분리
from sklearn.model_selection import train_test_split

x_train , x_test , y_train , y_test = train_test_split(x,y,random_state =1 , stratify=y)







# 정규화
from sklearn.preprocessing import MinMaxScaler

scaler = MinMaxScaler()
scaler.fit(x_train)

x_train_s = scaler.transform(x_train)
x_test_s = scaler.transform(x_test)





# xgboost 설치
# !pip install xgboost


# lightgbm 설치
# !pip install lightgbm





# 불러오기(분류)
from sklearn.neighbors import KNeighborsClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.linear_model import LogisticRegression # 로지스틱 회귀는 분류 모형임 
from sklearn.ensemble import RandomForestClassifier
from xgboost import XGBClassifier
from lightgbm import LGBMClassifier

# 성능예측 (Cross val score)
from sklearn.model_selection import cross_val_score
# 모델 튜닝
from sklearn.model_selection import GridSearchCV # CV = cross value

result = {} # 성능 정보 수집용 






# 성능예측
model = KNeighborsClassifier()

# 검증하기
cv_score = cross_val_score(model,x_train_s,y_train,cv=10)# 모델, 학습용 데이터 x y , 분할개수 cv
# 확인
print(cv_score) # 정확도 10개 : 분할학습 한결과 10개 
print('평균:', cv_score.mean())
print('표준편차:', cv_score.std())


# 결과수집
result = {}
result['KNN'] = cv_score.mean()





# 성능예측
model = DecisionTreeClassifier(max_depth=5,  random_state=1) # default = Accuracy

# 검증하기
cv_score = cross_val_score(model, x_train, y_train, cv=10)
# 확인
print(cv_score)
print('평균:', cv_score.mean()) # 정확도 
print('표준편차:', cv_score.std())



# 결과수집
result['Decision Tree'] = cv_score.mean()





# 성능예측
# 선언하기
model = LogisticRegression()

# 검증하기
cv_score = cross_val_score(model, x_train , y_train , cv = 10 )

# 확인
print(cv_score) # 정확도 10개 : 분할학습 한결과 10개 
print('평균:', cv_score.mean())
print('표준편차:', cv_score.std())


# 결과수집
result['Logistic Regression'] = cv_score.mean()





# 성능예측
model = RandomForestClassifier(max_depth = 5)

# 검증하기
cv_score = cross_val_score(model, x_train , y_train , cv = 10 )

# 확인
print(cv_score) # 정확도 10개 : 분할학습 한결과 10개 
print('평균:', cv_score.mean())
print('표준편차:', cv_score.std())



# 결과수집
result['Random Forest'] = cv_score.mean()





# 성능예측
model = XGBClassifier(max_depth =5)

# 검증하기
cv_score = cross_val_score(model, x_train , y_train , cv = 10 )

# 확인
print(cv_score) # 정확도 10개 : 분할학습 한결과 10개 
print('평균:', cv_score.mean())
print('표준편차:', cv_score.std())


# 결과수집
result['XGBoost'] = cv_score.mean()





# 성능예측
model = LGBMClassifier(max_depth = 5,verbose = -1, importance_type = 'gain') 

# 검증하기
cv_score = cross_val_score(model, x_train , y_train , cv = 10 )

# 확인
print(cv_score) # 정확도 10개 : 분할학습 한결과 10개 
print('평균:', cv_score.mean())
print('표준편차:', cv_score.std())


# 결과수집
result['LightGBM'] = cv_score.mean()





# 성능 비교
print(result)



# 성능 시각화 비교

# result 성능 비교 
plt.barh(y= result.keys() ,width=result.values())
plt.show()





# 파라미터
param = {'max_depth': range(1, 21),
         'n_estimators': range(60, 131, 10)}





# 학습하기(많은 시간이 소요될 수 있음)
model_grid = GridSearchCV(LGBMClassifier(verbose=-1), param ,cv= 5 , scoring ='accuracy')
model_grid.fit(x_train, y_train)




# 1. 최적 파라미터 출력
best_params = model_grid.best_params_ # 최적 파라미터 
print("Best Parameters:", best_params)

# 2. 최고 예측 성능 출력
best_score = model_grid.best_score_
print("Best Score:", best_score)


# 변수 중요도 시각화

# 학습된 최적 모델 가져오기
best_model = model_grid.best_estimator_

# 피처 중요도 추출
importances = best_model.feature_importances_

# 중요도를 데이터프레임으로 변환 (특성 이름과 중요도)
feature_names = x_train.columns if isinstance(x_train, pd.DataFrame) else [f"feature_{i}" for i in range(x_train.shape[1])]
feat_importances = pd.DataFrame({
    'Features': feature_names,
    'Importance': importances
})

# 중요도 기준으로 정렬
feat_importances = feat_importances.sort_values(by='Importance', ascending=False)

# 시각화
plt.figure(figsize=(10, 6))
sns.barplot(x='Importance', y='Features', data=feat_importances)
plt.title('Feature Importance')
plt.show()






# 예측하기
y_pred = best_model.predict(x_test)



# 성능평가
from sklearn.metrics import accuracy_score, f1_score, classification_report, confusion_matrix, ConfusionMatrixDisplay

print(confusion_matrix(y_test, y_pred))
print(classification_report(y_test, y_pred))


# 2. 정확도 평가
accuracy = accuracy_score(y_test, y_pred)
print("Accuracy:", accuracy)

# 3. F1 스코어 평가
f1 = f1_score(y_test, y_pred, average='weighted')  # 다중 클래스일 경우 'weighted'로 평균
print("F1 Score:", f1)

# 4. 분류 리포트 출력 (Precision, Recall, F1-score)
print("Classification Report:")
print(classification_report(y_test, y_pred))

# 5. 혼동 행렬
conf_matrix = confusion_matrix(y_test, y_pred)

# 6. 혼동 행렬 시각화
disp = ConfusionMatrixDisplay(confusion_matrix=conf_matrix)
disp.plot(cmap=plt.cm.Blues)
plt.title('Confusion Matrix')
plt.show()




