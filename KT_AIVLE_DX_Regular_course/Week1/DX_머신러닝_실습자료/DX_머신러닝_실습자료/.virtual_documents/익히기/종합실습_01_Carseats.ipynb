











# 라이브러리 불러오기
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import warnings

warnings.filterwarnings(action='ignore')
%config InlineBackend.figure_format = 'retina'


# 데이터 불러오기
path = 'https://raw.githubusercontent.com/jangrae/csv/master/Carseats.csv'
data = pd.read_csv(path)








# 데이터 살펴보기
data.head()




# 기술통계 확인
data.describe().T




# 변수 관련 정보 확인
data.info()










# x, y 분리
target = "Sales"

x= data.drop(columns = target)
y= data.loc[:,target]
 





# 가변수화
dummy_cols = ['ShelveLoc','Urban','US','Education']

x = pd.get_dummies(data = x, columns = dummy_cols, drop_first = True , dtype = int)

x





# 학습용, 평가용 데이터 분리
from sklearn.model_selection import train_test_split

x_train , x_test,y_train, y_test = train_test_split(x,y,random_state = 1,test_size= 0.3)






# 정규화
from sklearn.preprocessing import MinMaxScaler
scaler = MinMaxScaler()
scaler.fit(x_train)

x_train_s = scaler.transform(x_train)
x_test_s = scaler.transform(x_test)






# xgboost 설치
# !pip install xgboost


# lightgbm 설치
# !pip install lightgbm





# 라이브러리 불러오기(회귀 )
from sklearn.linear_model import LinearRegression
from sklearn.neighbors import KNeighborsRegressor
from sklearn.tree import DecisionTreeRegressor
from sklearn.ensemble import RandomForestRegressor
from xgboost import XGBRegressor
from lightgbm import LGBMRegressor

# 평가용 라이브러라
from sklearn.metrics import *






# 선언하기
model_linear = LinearRegression()

# 학습하기
model_linear.fit(x_train, y_train)

# 예측하기
y_pred_linear = model_linear.predict(x_test)

# 평가하기
print('MAE :', mean_absolute_error(y_test,y_pred_linear))
print('R2-score :', r2_score(y_test,y_pred_linear))





# 선언하기
model_knn = KNeighborsRegressor(n_neighbors=5)

# 학습하기  * 단 정규화된 데이터를 학습  
model_knn.fit(x_train_s, y_train)

# 예측하기
y_pred_knn = model_knn.predict(x_test_s)

# 평가하기
print('MAE :', mean_absolute_error(y_test,y_pred_knn))
print('R2-score :', r2_score(y_test,y_pred_knn))






# 선언하기
model_dt = DecisionTreeRegressor(max_depth = 5)

# 학습하기
model_dt.fit(x_train,y_train)

# 예측하기
y_pred_dt = model_dt.predict(x_test)

# 평가하기
print('MAE :', mean_absolute_error(y_test,y_pred_dt))
print('R2-score :', r2_score(y_test,y_pred_dt))






# 선언하기
model_rf = RandomForestRegressor(max_depth = 5)

# 학습하기
model_rf.fit(x_train, y_train)

# 예측하기
y_pred_rf =model_rf.predict(x_test)

# 평가하기
print('MAE :', mean_absolute_error(y_test,y_pred_rf))
print('R2-score :', r2_score(y_test,y_pred_rf))







# 선언하기
model_xgb = XGBRegressor(max_depth =5)
# 학습하기
model_xgb.fit(x_train, y_train)

# 예측하기
y_pred_xgb =model_xgb.predict(x_test)

# 평가하기
print('MAE :', mean_absolute_error(y_test,y_pred_xgb))
print('R2-score :', r2_score(y_test,y_pred_xgb))







# 선언하기

model_lgm = LGBMRegressor(max_depth = 5,verbose = -1, importance_type = 'gain') 
# 학습하기
model_lgm.fit(x_train, y_train) # 좀더 복잡하게 모델 생성함  > verbose = -1 하면 학습 과정을 굳이 출력 안해줌

# 예측하기
y_pred_lgm = model_lgm.predict(x_test)

# 평가하기
print('MAE :', mean_absolute_error(y_test,y_pred_lgm))
print('R2-score :', r2_score(y_test,y_pred_lgm))



