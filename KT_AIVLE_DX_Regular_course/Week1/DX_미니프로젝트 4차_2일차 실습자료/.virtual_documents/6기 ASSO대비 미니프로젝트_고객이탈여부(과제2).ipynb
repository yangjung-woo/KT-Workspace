

















# 여기에 답안코드를 작성하세요.

import sklearn as sk








# 여기에 답안코드를 작성하세요.
import pandas as pd








# 여기에 답안코드를 작성하세요.
df = pd.read_csv('churn_data.csv') 
df








# 여기에 답안코드를 작성하세요.
drop_cols =['customerID']
df = df.drop(columns = drop_cols)








# 여기에 답안코드를 작성하세요.
print(df['TotalCharges'].dtype) # Object 타입 

df['TotalCharges'] = df['TotalCharges'].replace(' ',0).astype(float)

df2 = df.copy()








# 여기에 답안코드를 작성하세요.
print(df2['Churn'].value_counts()) # 데이터별 개수 확인

df2['Churn'] = df['Churn'].map({'Yes':1 , 'No':0})
df3 = df2.copy()


df3['Churn'].value_counts() # 변경 잘 되었는지 확인





# 여기에 답안코드를 작성하세요.
df4 = df3.copy()

df4.isna().sum() # DeviceProtection 은 40%이상이 결측치 > 열 제거 
# 40% 이상 결측 컬럼 삭제 
drop_cols = ['DeviceProtection']
df4= df4.drop(columns = drop_cols)

# 40이하 결측 행 삭제
df4 = df4.dropna(axis = 0)
#확인용
df4.isna().sum()










# 여기에 답안코드를 작성하세요.
import seaborn as sns
import matplotlib.pyplot as plt

sns.countplot(x='SeniorCitizen' , data =df4)# bar 차트로 어떻게 불균형을 확인?? > Countplot을 사용하라는 의미 
# box plot을 말한건가?
#sns.boxplot(x='SeniorCitizen' , data =df4)

drop_cols = 'SeniorCitizen'

df4= df4.drop(columns= drop_cols)










# 여기에 답안코드를 작성하세요.

sns.histplot(x = 'tenure', hue = 'Churn', data= df4,kde=True)
print('Q. 서비스 사용기간이 길어질 수록 이탈이 적다?')
print('A. O')



# 히트맵 시각화 

corr_df=df4[['tenure','MonthlyCharges','TotalCharges']].corr()

sns.heatmap(corr_df, annot = True)

print('가장 높은 상관계수: 0.83')








# 여기에 답안코드를 작성하세요.
df4.info() # object 타입 확인 
# print(df4.columns) 컬럼 한번에 복붙하려고 출력
dummy_cols = ['gender', 'Partner', 'Dependents', 'PhoneService',
       'MultipleLines', 'InternetService', 'OnlineSecurity', 'OnlineBackup',
       'TechSupport', 'StreamingTV', 'StreamingMovies', 'Contract',
       'PaperlessBilling', 'PaymentMethod']

df5  = pd.get_dummies(data= df4, columns = dummy_cols , drop_first = True)



df5.head(1) # 가변수화 잘 되었는지 확인용








# 여기에 답안코드를 작성하세요.

from sklearn.model_selection import train_test_split 

target = 'Churn'
y = df5[target]
X = df5.drop(columns = target)

X_train, X_valid, y_train, y_valid = train_test_split(X,y,random_state=42, stratify = y , test_size = 0.2)









# 여기에 답안코드를 작성하세요.
from sklearn.preprocessing import MinMaxScaler

scaler = MinMaxScaler()

X_train = scaler.fit_transform(X_train)
X_valid= scaler.transform(X_valid)








# 여기에 답안코드를 작성하세요.
from sklearn.linear_model import LogisticRegression

lg_model = LogisticRegression()

lg_model.fit(X_train,y_train)



# 여기에 답안코드를 작성하세요.
from sklearn.neighbors import KNeighborsClassifier

KNN_model = KNeighborsClassifier(n_neighbors =5)

KNN_model.fit(X_train,y_train)


# 여기에 답안코드를 작성하세요.
from sklearn.tree import DecisionTreeClassifier

dt_model = DecisionTreeClassifier(max_depth=10, random_state=42)
dt_model.fit(X_train, y_train)


# 여기에 답안코드를 작성하세요.
from sklearn.ensemble import RandomForestClassifier
rf_model = RandomForestClassifier(n_estimators=3, random_state=42)
rf_model.fit(X_train, y_train)


# 여기에 답안코드를 작성하세요.
from xgboost import XGBClassifier

xgb_model = XGBClassifier(n_estimators=3, random_state=42)
xgb_model.fit(X_train, y_train)


# 여기에 답안코드를 작성하세요.
from lightgbm import LGBMClassifier 
lgb_model = LGBMClassifier(n_estimators=3, random_state=42)
lgb_model.fit(X_train, y_train)





# 여기에 답안코드를 작성하세요.
from sklearn.metrics import * 
y_pred = lgb_model.predict(X_valid)

print(confusion_matrix(y_pred,y_valid))
# 히트맵 시각화 
sns.heatmap(confusion_matrix(y_pred,y_valid) , annot = True )
print(classification_report(y_pred,y_valid))





import tensorflow as tf
from tensorflow.keras.models import Sequential, load_model
from tensorflow.keras.layers import Dense, Activation, Dropout, BatchNormalization, Input
from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint
from tensorflow.keras.utils import to_categorical

tf.random.set_seed(1)





n = X_train.shape[1]
n


# 여기에 답안코드를 작성하세요.
n = X_train.shape[1]

model = Sequential([Input(shape =(n,)),
                    Dense(27, activation = 'relu'),
                    Dropout(0.2),
                    Dense(9, activation = 'relu'),
                    Dropout(0.2),
                    Dense(3, activation = 'relu'),
                    Dropout(0.2),
                    Dense(1, activation = 'sigmoid'),
                    
    
])
model.compile(optimizer = 'adam' ,
              loss = 'binary_crossentropy',
              metrics = ['Accuracy'])

history = model.fit(X_train,y_train,
            epochs = 30,
              batch_size = 16,
              validation_data = (X_valid,y_valid),
                   )









print(history.history.keys())  # 사용 가능한 


# 여기에 답안코드를 작성하세요.
train_acc = history.history['Accuracy']
val_acc = history.history['val_Accuracy']

ep = range(1,len(train_acc)+1)

plt.plot(ep, train_acc, label='acc')
plt.plot(ep, val_acc, label='val_acc')
plt.title('Accuracy')
plt.xlabel('Epochs')
plt.ylabel('Acc')
plt.legend()
plt.show()





# 딥러닝 모델 예측값 확인 
import numpy as np
y_pred = model.predict(X_valid)
y_pred = np.where(y_pred >=0.5 ,1,0)

print(classification_report(y_pred,y_valid))
print(confusion_matrix(y_pred,y_valid))



