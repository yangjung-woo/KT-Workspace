








!pip install plotly
import plotly.graph_objects as go


# 기본 라이브러리 가져오기
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

from sklearn.model_selection import train_test_split
from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import *

from sklearn.datasets import load_breast_cancer, load_digits, load_iris, make_swiss_roll
from sklearn.preprocessing import MinMaxScaler

from sklearn.decomposition import PCA





# 럭비공 형태의 샘플 데이터 생성 함수
def generate_rugby_data(n_points=1000, a=1, b=1.5, c=2):
    phi = np.random.uniform(0, np.pi, n_points)
    theta = np.random.uniform(0, 2*np.pi, n_points)
    x = a * np.sin(phi) * np.cos(theta)
    y = b * np.sin(phi) * np.sin(theta)
    z = c * np.cos(phi)
    X = np.column_stack((x, y, z))
    return X

rugby = generate_rugby_data()

# 스위스롤 데이터
swiss_roll, _ = make_swiss_roll(n_samples=1000, noise=0.2)


# 3차원 스캐터 함수 생성
def my_3d_Scatter(X) :
    fig = go.Figure()
    fig.add_trace(go.Scatter3d(x=X[:, 0], y=X[:, 1], z=X[:, 2],
                           mode='markers', marker=dict(size=2, color='blue'),
                           name='Original Data'))

    fig.update_layout(margin=dict(l=0, r=0, b=0, t=0),
                      scene=dict(xaxis_title='X Axis', yaxis_title='Y Axis', zaxis_title='Z Axis'))

    fig.show()











my_3d_Scatter(rugby)





# PCA를 이용하여 2개의 주성분으로 차원 축소
pca = PCA(n_components=2)
X_pca = pca.fit_transform(rugby)

# PCA 축소 데이터 조회
plt.scatter(X_pca[:, 0], X_pca[:, 1])
plt.grid()
plt.show()



# PCA를 이용하여 2개의 주성분으로 차원 축소
pca = PCA(n_components=3)
X_pca = pca.fit_transform(rugby)

# PCA 축소 데이터 조회
plt.scatter(X_pca[:, 0], X_pca[:, 1])
plt.grid()
plt.show()








my_3d_Scatter(swiss_roll)





# PCA를 이용하여 2개의 주성분으로 차원 축소
pca = PCA(n_components=2)
# n_components=2는 PCA가 데이터에서 두 개의 주성분을 선택하여 데이터의 차원을 
# 2차원으로 축소
X_pca = pca.fit_transform(swiss_roll)

# PCA 축소 데이터 조회
plt.scatter(X_pca[:, 0], X_pca[:, 1])
plt.grid()
plt.show()












iris = pd.read_csv("https://raw.githubusercontent.com/DA4BAM/dataset/master/iris.csv")
target = 'Species'
x = iris.drop(target, axis = 1)
y = iris.loc[:, target]


x.head()





scaler = MinMaxScaler()
x2 = scaler.fit_transform(x)

# (옵션)데이터프레임 변환
x2 = pd.DataFrame(x2, columns= x.columns)


x2.head()





from sklearn.decomposition import PCA


# feature 수
x2.shape[1] # 4차원





# 주성분 수 2개
n = 2
pca = PCA(n_components = n)

# 만들고, 적용하기(결과는 넘파이 어레이)
x2_pc = pca.fit_transform(x2)


# 2개의 주성분
x2_pc[:5]


# (옵션) 데이터프레임으로 변환
x2_pc = pd.DataFrame(x2_pc, columns = ['PC1', 'PC2'])
x2_pc.head()





pd.concat([iris, x2_pc], axis = 1).head()





sns.scatterplot(x = 'PC1', y = 'PC2', data = x2_pc, hue = y)
plt.grid()
plt.show()











# breast_cancer 데이터 로딩
cancer=load_breast_cancer()
x = cancer.data
y = cancer.target

x = pd.DataFrame(x, columns=cancer.feature_names)

x.shape


x.head()


x.info()


x.describe().T





scaler = MinMaxScaler()
x = scaler.fit_transform(x)

# (옵션)데이터프레임 변환
x = pd.DataFrame(x, columns=cancer.feature_names)


y # y = 1 악성, 0 약성  








x_train, x_val, y_train, y_val = train_test_split(x, y, test_size = .3, random_state = 20)





from sklearn.decomposition import PCA


# feature 수
x_train.shape[1]





# 주성분을 몇개로 할지 결정(최대값 : 전체 feature 수)
n = x_train.shape[1]

# 주성분 분석 선언
pca = PCA(n_components=n)

# 만들고, 적용하기
x_train_pc = pca.fit_transform(x_train)
x_val_pc = pca.transform(x_val) # transform 만 적용 지도학습이라면 구분이 필요하지만 비지도의 경우 구분 x 

#검증 데이터에 대해 fit을 하지 않는 이유는 데이터 누출을 방지하고, 
# 일관성 있는 차원 축소를 보장하기 위함입니다. 





# 칼럼이름 생성
column_names = [ 'PC'+str(i+1) for i in range(n) ]
column_names


# 데이터프레임으로 변환하기
x_train_pc = pd.DataFrame(x_train_pc, columns = column_names)
x_val_pc = pd.DataFrame(x_val_pc, columns = column_names)
x_train_pc








# 주성분 1개짜리
n = 1
pca1= PCA(n_components=n)

# 만들고, 적용하기
x_train_pc1 = pca1.fit_transform(x_train)
x_val_pc1 = pca1.transform(x_val)

# 칼럼이름 생성
column_names = [ 'PC'+str(i+1) for i in range(n) ]
column_names

x_train_pc1 = pd.DataFrame(x_train_pc1, columns = column_names)
x_val_pc1 = pd.DataFrame(x_val_pc1, columns = column_names)
x_train_pc1


# 주성분 2개짜리
n=2
pca2 = PCA(n_components=n)

# 만들고, 적용하기
x_train_pc2 = pca2.fit_transform(x_train)
x_val_pc2 = pca2.transform(x_val)

# 칼럼이름 생성
column_names = [ 'PC'+str(i+1) for i in range(n) ]
column_names

x_train_pc2 = pd.DataFrame(x_train_pc2, columns = column_names)
x_val_pc2 = pd.DataFrame(x_val_pc2, columns = column_names)
x_train_pc2


# 주성분 3개짜리
n = 3
pca3 = PCA(n_components=n)

# 만들고, 적용하기
x_train_pc3 = pca3.fit_transform(x_train)
x_val_pc3 = pca3.transform(x_val)

# 칼럼이름 생성
column_names = [ 'PC'+str(i+1) for i in range(n) ]
column_names

x_train_pc3 = pd.DataFrame(x_train_pc3, columns = column_names)
x_val_pc3 = pd.DataFrame(x_val_pc3, columns = column_names)
x_train_pc3





print(x_train_pc1[:3])
print(x_train_pc2[:3])
print(x_train_pc3[:3])
# 항상 첫번째 축이 분산이 가장 큼





n = x_train.shape[1]


plt.plot(range(1,n+1), pca.explained_variance_ratio_, marker = '.')
plt.xlabel('No. of PC')
plt.grid()
plt.show()





# 대부분의 상황에서 주성분은 2~3개가 적절함





sns.scatterplot(x = 'PC1', y = 'PC2', data = x_train_pc, hue = y_train)
plt.grid()
plt.show()











model0 = KNeighborsClassifier()
model0.fit(x_train, y_train)





# 원본데이터 모델의 성능
pred0 = model0.predict(x_val)

print(confusion_matrix(y_val, pred0))
print(accuracy_score(y_val, pred0))
print(classification_report(y_val, pred0))








cols = column_names[:1]
x_train_pc1 = x_train_pc.loc[:, cols]
x_val_pc1 = x_val_pc.loc[:, cols]


x_train_pc1.shape


x_train_pc1.head()





# KNN 모델링, 주성분 1개로 모델링
model_pc1 = KNeighborsClassifier()
model_pc1.fit(x_train_pc1, y_train)
# 주성분 1개 데이터 모델의 성능
y_pred_pc1 = model_pc1.predict(x_val_pc1)

# 성능 평가 
print(confusion_matrix(y_val, y_pred_pc1))
print(accuracy_score(y_val, y_pred_pc1))
print(classification_report(y_val, y_pred_pc1))



# 원본데이터 모델의 성능
model0 = KNeighborsClassifier()
model0.fit(x_train, y_train)

# 원본데이터 모델의 성능
pred0 = model0.predict(x_val)

print(confusion_matrix(y_val, pred0))
print(accuracy_score(y_val, pred0))
print(classification_report(y_val, pred0))


x_train.head()





n = 2
# 데이터 준비
cols = column_names[:n]
x_train_pc_n = x_train_pc.loc[:, cols]
x_val_pc_n = x_val_pc.loc[:, cols]

# 모델링
model = KNeighborsClassifier()
model.fit(x_train_pc_n, y_train)

# 예측
pred = model.predict(x_val_pc_n)

# 평가
print(confusion_matrix(y_val, pred))
print(accuracy_score(y_val, pred))
print(classification_report(y_val, pred))






n = 3
# 데이터 준비
cols = column_names[:n]
x_train_pc_n = x_train_pc.loc[:, cols]
x_val_pc_n = x_val_pc.loc[:, cols]

# 모델링
model = KNeighborsClassifier()
model.fit(x_train_pc_n, y_train)

# 예측
pred = model.predict(x_val_pc_n)

# 평가
print(confusion_matrix(y_val, pred))
print(accuracy_score(y_val, pred))
print(classification_report(y_val, pred))





result = []
for n in range(1,31):
    cols = column_names[:n]
    x_train_pc_n = x_train_pc.loc[:, cols]
    x_val_pc_n = x_val_pc.loc[:, cols]
    
    # 모델링
    model = KNeighborsClassifier()
    model.fit(x_train_pc_n, y_train)
    
    # 예측
    pred = model.predict(x_val_pc_n)
    
    # 평가
    result.append(accuracy_score(y_val,pred))
    


result


# 시각화 
plt.plot(range(1,31),result, marker ='.')
plt.axhline(accuracy_score(y_val,pred0) ,color ='r')
plt.grid()
plt.show()








from sklearn.manifold import TSNE


# 2차원으로 축소하기
tsne = TSNE(n_components = 2, random_state=20)
x_tsne = tsne.fit_transform(x)

# 사용의 편리함을 위해 DataFrame으로 변환
x_tsne = pd.DataFrame(x_tsne, columns = ['T1','T2'])


x_tsne.shape





plt.figure(figsize=(6,6))
sns.scatterplot(x = 'T1', y = 'T2', data = x_tsne, hue = y)
plt.grid()











digits = load_digits()
x = digits.data
y = digits.target

y = pd.Categorical(y)


x.shape





print(x[0].reshape(8,8))


# f, axes = plt.subplots(5, 2, sharey=True, figsize=(16,6))
plt.figure(figsize=(10, 4))
for i in range(10):
    plt.subplot(2, 5, i + 1)
    plt.imshow(x[i,:].reshape([8,8]), cmap='gray');





# 최대, 최소값
np.min(x), np.max(x)


# 최대값으로 나누면 Min Max 스케일링이 됩니다.
x = x / 16





# 차원 축소
pca = PCA(n_components=2)
x_pca = pca.fit_transform(x)

# 데이터프레임으로 변환(옵션)
x_pca = pd.DataFrame(x_pca, columns = ['PC1', 'PC2'])


# 시각화
plt.figure(figsize=(8, 8))
sns.scatterplot(x = 'PC1', y = 'PC2', data = x_pca, hue = y)
plt.grid()
plt.show()





tsne = TSNE(n_components = 2, random_state=20)
x_tsne = tsne.fit_transform(x)

# 데이터프레임으로 변환(옵션)
x_tsne = pd.DataFrame(x_tsne, columns = ['T1', 'T2'])


# 시각화
plt.figure(figsize=(8, 8))
sns.scatterplot(x = 'T1', y = 'T2', data = x_tsne, hue = y)
plt.grid()
plt.show()



