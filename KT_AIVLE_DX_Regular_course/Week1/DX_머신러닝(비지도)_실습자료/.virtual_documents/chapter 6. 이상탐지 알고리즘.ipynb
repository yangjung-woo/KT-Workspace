





import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

from sklearn.model_selection import train_test_split
from sklearn.ensemble import IsolationForest # Isolation Forest!
from sklearn.metrics import *

from tqdm import tqdm
import warnings
warnings.simplefilter(action='ignore')








# Single Blob
X1 = pd.read_csv('https://raw.githubusercontent.com/DA4BAM/dataset/master/Anomaly_X.csv')

# Double Blob
X2 = pd.read_csv('https://raw.githubusercontent.com/DA4BAM/dataset/master/Anomaly_X2.csv')








def model_visualize(model, v1, v2, title = "") :
    # 메쉬그리드값 저장하기
    xx, yy = np.meshgrid(np.linspace(-5, 5, 50), np.linspace(-5, 5, 50)) # mesh grid

    # 메쉬 그리드값에 대해 모델 부터 Anomaly Score 만들기.
    Z = model.decision_function(np.c_[xx.ravel(), yy.ravel()]) # Anomaly Score
    Z = Z.reshape(xx.shape)
    # 시각화
    plt.figure(figsize = (8,8))
    plt.title(title)

    # 메쉬그리드 값의 Anomaly Score에 대한 등고선
    plt.contourf(xx, yy, Z, cmap=plt.cm.Blues_r)

    # 데이터 산점도 그리기.(예측 결과 Abnormal은 오렌지색, Normal은 흰색)
    sns.scatterplot(x=v1, y=v2, sizes = 30, edgecolor='k', hue = pred, palette=['white', 'orange'])

    plt.axis("tight")
    plt.xlim(-5, 5)
    plt.ylim(-5, 5)
    plt.show()





# sample data
plt.figure(figsize = (8,8))

plt.scatter(X1['v1'], X1['v2'], c="white", s=30, edgecolor="k")

plt.xlim(-5, 5)
plt.ylim(-5, 5)
plt.grid()
plt.show()





# 모델링
model = IsolationForest(contamination = 0.1, n_estimators = 50 )
model.fit(X1)
pred = model.predict(X1)
pred


pred = np.where(pred == 1, 0, 1)





model_visualize(model, X1['v1'], X1['v2'], 'Isolation Forest')





# 여기를 조정해 봅시다.
cont = 0.03 # 이상치로 간주 할 비율 
n_est = 150

# 모델링
model = IsolationForest(contamination = cont, n_estimators = n_est
                        , random_state = 20)
model.fit(X1)
pred = model.predict(X1)
pred = np.where(pred == 1, 0, 1)

model_visualize(model, X1['v1'], X1['v2'], 'Isolation Forest')





plt.figure(figsize = (8,8))

plt.scatter(X2['v1'], X2['v2'], c="white", s=30, edgecolor="k")

plt.xlim(-5, 5)
plt.ylim(-5, 5)
plt.grid()
plt.show()





# 모델링
model = IsolationForest(contamination = 0.1, n_estimators = 50 )
model.fit(X2)
pred = model.predict(X2)
pred = np.where(pred == 1, 0, 1)





model_visualize(model, X2['v1'], X2['v2'], 'Isolation Forest')





# 여기를 조정해 봅시다.
cont = 0.08
n_est = 500

# 
# 모델링
model = IsolationForest(contamination = cont, n_estimators = n_est, random_state = 20)
model.fit(X2)
pred = model.predict(X2)
pred = np.where(pred == 1, 0, 1)

# 모델 시각화
model_visualize(model, X2['v1'], X2['v2'], 'Isolation Forest')

















path = "https://raw.githubusercontent.com/DA4BAM/dataset/master/secom_9.csv"
data = pd.read_csv(path)

data['label'] = 0
data.loc[data['defeat']== 'defeat', 'label']= 1
data.drop(['datetime','defeat'], axis = 1, inplace=True)
data.head()





target = 'label'


data[target].value_counts() / data.shape[0]





x = data.drop(target, axis = 1)
y = data.loc[:, target]





from sklearn.model_selection import train_test_split


# train_val에서 train : val = 8 : 2
x_train, x_val, y_train, y_val = train_test_split(x, y, test_size=0.2, random_state = 20)


print(x_train.shape, x_val.shape)








y_train.value_counts(normalize = True)


# contamination=0.068로 지정하고 모델을 생성해 봅시다.
model = IsolationForest(contamination=0.07, random_state = 20)

# 비지도 학습입니다. x_train만 사용!
model.fit(x_train) # IsolationForest( X_train)은 정상 , 비정상 데이터 모두 포함되어있음





# 예측
pred = model.predict(x_val)

# 결과를 1,0으로 변환(np.where)
pred = np.where(pred == 1, 0, 1)

# 분류 결과를 평가해 봅시다.
print(confusion_matrix(y_val, pred))
print(classification_report(y_val, pred))





model = IsolationForest(contamination=0.07, random_state = 20)

# 비지도 학습입니다. x_train만 사용!
model.fit(x_train) # IsolationForest( X_train)은 정상 , 비정상 데이터 모두 포함되어있음

pred = model.predict(x_val)

# 결과를 1,0으로 변환(np.where)
pred = np.where(pred == 1, 0, 1)

# 분류 결과를 평가해 봅시다.
print(confusion_matrix(y_val, pred))
print(classification_report(y_val, pred))


# 그리드 서치 
from sklearn.model_selection import GridSearchCV # CV = cross value

param = {'contamination': np.arange(0.01, 1, 0.01),
         'n_estimators': range(100,200,1)}
model = GridSearchCV(IsolationForest(random_state=20), # 기본 모델
                     param,                 # 파라미터 범위
                     cv = 5,
                    scoring ='f1')  
model.fit(x_train)
model.best_params_





model1 = IsolationForest(contamination=0.07, random_state = 20)
# 비지도 학습입니다. x_train만 사용!
model1.fit(x_train) # IsolationForest( X_train)은 정상 , 비정상 데이터 모두 포함되어있음
pred1 = model.predict(x_val)
# 결과를 1,0으로 변환(np.where)
pred1 = np.where(pred == 1, 0, 1)
cm1 = confusion_matrix(y_val,pred1)


model2 = IsolationForest(contamination=0.07, random_state = 20)
# 비지도 학습입니다. x_train만 사용!
model2.fit(x_train) # IsolationForest( X_train)은 정상 , 비정상 데이터 모두 포함되어있음
pred2 = model.predict(x_val)
# 결과를 1,0으로 변환(np.where)
pred2 = np.where(pred == 1, 0, 1)
cm2 = confusion_matrix(y_val,pred2)


print(cm1)
print(cm2)


# 비즈니스 가치 matrix : 제2종 오류의 가중치가 가장 큼 
bv = np.array([[0,10][100,10]])
bv


np.sum(cm1 *bv) , np.sum(cm2*bv)








# 학습 데이터에 대한 abnormal 점수 계산
tr_score = model.score_samples(x_train)

# 0~1 의 값으로 score 바꾸기
tr_score = -1 * tr_score

# score의 분포 살펴보기
sns.histplot(tr_score, kde= True)
plt.show()








from sklearn.metrics import precision_recall_curve

def prec_rec_f1_curve(y, score, pos = 1) :

    # precision, recall, f1 계산
    precision, recall, thresholds  = precision_recall_curve(y, score, pos_label=1)
    f1 = 2 / (1/precision + 1/recall)

    # 그래프 그리기
    plt.figure(figsize = (12,8))
    plt.plot(thresholds, np.delete(precision, -1), label = 'precision')
    plt.plot(thresholds, np.delete(recall, -1), label = 'recall')
    plt.plot(thresholds, np.delete(f1, -1), label = 'f1')

    #-------------------------------------------------------------------------
    # 아래 코드는 차트 꾸미기 입니다.
    # f1를 최대화 해주는 threshold
    thre = round(thresholds[np.argmax(f1)],4)
    f1s = round(max(f1),4)
    plt.axvline(thre , color = 'darkred', linewidth = .7)
    plt.axhline( f1s, color = 'darkred', linewidth = .7)
    plt.text(thre, .5, thre, color = 'darkred')
    plt.text(min(thresholds), f1s, f1s, color = 'darkred')

    plt.xlabel('Anomaly Score')
    plt.ylabel('Performance')
    plt.legend()
    plt.grid()
    plt.show()

    return precision, recall, f1, thresholds


# validation score 계산
va_score = model.score_samples(x_val)

# 0~1 의 값으로 score 바꾸기
va_score = -1 * va_score

# prec_rec_f1_curve 이용하여 최적의 cutoff 찾기

_, _, f1, thresholds = prec_rec_f1_curve(y_val, va_score)





cutoff_f1max = thresholds[np.argmax(f1)]


pred = np.where(va_score >= cutoff_f1max,1,0)


print(confusion_matrix(y_val, pred))
print(classification_report(y_val, pred))








new_cont = sum(va_score >= cutoff_f1max) / len(va_score)
new_cont





n_est = range(50,501,50)
f1_list = []
for t in tqdm(n_est) :
    f1_temp = []
    for i in range(5) :
        model = IsolationForest(contamination = new_cont, n_estimators = t)
        model.fit(x_train)
        pred = model.predict(x_val)
        pred = np.where(pred == 1, 0, 1)
        f1_temp.append(f1_score(y_val, pred, pos_label=1))

    f1_list.append(np.mean(f1_temp))


plt.figure(figsize=(12, 8))
plt.plot(n_est, f1_list, marker = '.')
plt.ylabel('F1 Score')
plt.xlabel('n_estimators')
plt.grid()
plt.show()





m_sam = range(500,5001,500)
f1_list = []
for s in tqdm(m_sam) :
    f1_temp = []
    for i in range(5) :
        model = IsolationForest(contamination = new_cont, n_estimators = 300 , max_samples = s)
        model.fit(x_train)
        pred = model.predict(x_val)
        pred = np.where(pred == 1, 0, 1)
        f1_temp.append(f1_score(y_val, pred, pos_label=1))

    f1_list.append(np.mean(f1_temp))



plt.figure(figsize=(12, 8))
plt.plot(m_sam, f1_list, marker = '.')
plt.ylabel('F1 Score')
plt.xlabel('max_samples')
plt.grid()
plt.show()
