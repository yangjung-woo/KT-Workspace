

















import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from tqdm.auto import tqdm
import warnings
warnings.filterwarnings('ignore')

import joblib

# 필요한 라이브러리, 함수 로딩 ------------------
from sklearn.tree import DecisionTreeClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC
from sklearn.model_selection import GridSearchCV
from sklearn.preprocessing import MinMaxScaler
from xgboost import XGBClassifier
from lightgbm import LGBMClassifier
from sklearn.metrics import *
# 하이퍼 파라미터 튜닝은 _val로 진행
# 모델 성능 비교는 data_test로 진행
# 변수를 삭제?  > 타당한 근거가 있어야 함  ex) Tree계열 알고리즘은 변수가 많아도 안정적인 성능
#                                               LGBM ,로지스틱 회귀 , KNN은 변수가 많아지거나 이상한 변수가 추가되면 성능하락
#                                               옵션(주성분 분석)






# 변수의 특성 중요도 계산하기
def plot_feature_importance(importance, names, result_only = False, topn = 'all'):
    feature_importance = np.array(importance)
    feature_name = np.array(names)
    
    data={'feature_name':feature_name,'feature_importance':feature_importance}
    fi_temp = pd.DataFrame(data)

    #변수의 특성 중요도 순으로 정렬하기
    fi_temp.sort_values(by=['feature_importance'], ascending=False,inplace=True)
    fi_temp.reset_index(drop=True, inplace = True)

    if topn == 'all' :
        fi_df = fi_temp.copy()
    else :
        fi_df = fi_temp.iloc[:topn]

    #변수의 특성 중요도 그래프로 그리기
    if result_only == False :
        plt.figure(figsize=(10,20))
        sns.barplot(x='feature_importance', y='feature_name', data = fi_df)

        plt.xlabel('importance')
        plt.ylabel('feature name')
        plt.grid()

    return fi_df











data_train = pd.read_csv('data01_train.csv')


data_test = pd.read_csv('data01_test.csv')


drop_cols = 'subject'
data_test.drop(columns = drop_cols ,inplace =True)
data_train.drop(columns = drop_cols, inplace = True)





print(data_train.shape)
print(data_test.shape)


data_test.info() 


# Acrivity 범주의 종류 확인
categories_train = data_train['Activity'].unique()
categories_test = data_test['Activity'].unique()
print("train 데이터 범주의 종류: \n", categories_train)
print("test 데이터 범주의 종류: \n", categories_train)


# 범주별 빈도수 확인
activity_count_train = data_train['Activity'].value_counts()
activity_count_test = data_test['Activity'].value_counts()
print("train 데이터 범주별 빈도수: \n", activity_count_train)
print("test 데이터 범주별 빈도수: \n", activity_count_test)


# 범주별 비율 계산
activity_ratio_train = data_train['Activity'].value_counts(normalize=True)
activity_ratio_test = data_test['Activity'].value_counts(normalize=True)
print("train 데이터 범주별 비율(%) \n", activity_ratio_train)
print("test 데이터 범주별 비율(%) \n", activity_ratio_test)











target = 'Activity'
x = data_train.drop(columns= target)
y = data_train.loc[:,target]









from sklearn.model_selection import train_test_split

x_train , x_val , y_train , y_val = train_test_split(x,y,random_state=42,test_size=0.2)








from sklearn.preprocessing import MinMaxScaler
scaler = MinMaxScaler()
x_train = scaler.fit_transform(x_train) # 학습용 데이터는 fit 학습을 수행하고 변환하지만
x_val= scaler.transform(x_val)# 검증용 데이터는 fit 을 수행해선 안됀다 > 평가용 데이터 역시 fit을 해선 안됀다 









# 사전작업  y label은 연속형 변수로 라벨링해야합니다
# 방법1. # 레이블 인코딩
# from sklearn.preprocessing import LabelEncoder
# label_encoder = LabelEncoder()
# y_train_encoded = label_encoder.fit_transform(y_train)
# y_val_encoded = label_encoder.fit_transform(y_val)

#방법 2  대체 
activity_mapping = {
    'LAYING': 0,
    'STANDING': 1,
    'SITTING': 2,
    'WALKING': 3,
    'WALKING_UPSTAIRS': 4,
    'WALKING_DOWNSTAIRS': 5
}
y_train = y_train.map(activity_mapping)
y_val = y_val.map(activity_mapping)











# KNN 근접 이웃 방식 
# 찾아야하는 param : n_neighbors(초기 n값 ) =? , weights(가중치 방식) = ?  , metric(거리계산 방식) =?  
param_grid = {
    'n_neighbors': range(1,10,2),  #홀수를 자주 사용하는 이유는 다수결 투표에서 동률을 피하기 위해서
    'weights': ['uniform', 'distance'],  # 가중치 방식
    'metric': ['euclidean', 'manhattan', 'minkowski']  # 거리 계산 방식
}
# 모델 생성 
knn = KNeighborsClassifier()

# GridSearchCV를 사용한 하이퍼파라미터 튜닝
grid_search = GridSearchCV(knn, param_grid, cv=5, scoring='accuracy') # n_jobs = -1 : 모든 CPU가동 병렬처리> 속도향상 하지만 유료 서비스에선 비용발생 
grid_search.fit(x_train, y_train) # x는 반드시 스케일링 

# 최적의 하이퍼파라미터 출력
print(f'최적의 파라미터: {grid_search.best_params_}')


# 최적의 하이퍼파라미터로 학습된 모델을 사용하여 예측
best_knn = grid_search.best_estimator_
y_pred = best_knn.predict(x_val)

# 정확도 평가
print(confusion_matrix(y_val, y_pred))
print('Classification Report  \n' , classification_report(y_val, y_pred)) #


# 최적의 파라미터는 {'metric': 'manhattan', 'n_neighbors': 3, 'weights': 'distance'}


# 시간 제약 {'metric': 'euclidean', 'n_neighbors': 1}








# 서포트 벡터 머신
param_grid = {
    #'C': [0.1, 1, 10, 100],  # 규제 강도, 작은 값은 과적합을 방지하고 큰 값은 과적합을 허용
    #'kernel': ['linear', 'rbf', 'poly'],  # 커널 함수 (linear, RBF, polynomial)
    #'gamma': ['scale', 'auto', 0.1, 1, 10],  # RBF 커널에서 중요한 파라미터, 다른 커널에서는 무시됨
    'C' : [10],
    'kernel' : ['rbf'],
    'gamma': [0.1]  # RBF 커널에서 중요한 파라미터, 다른 커널에서는 무시됨
}
# 모델 생성 
svm = SVC()

# GridSearchCV를 사용한 하이퍼파라미터 튜닝
grid_search = GridSearchCV(svm, param_grid, cv=5, scoring='accuracy', n_jobs=1)
grid_search.fit(x_train, y_train)

# 최적의 하이퍼파라미터 출력
print(f'최적의 파라미터: {grid_search.best_params_}')

# 최적의 하이퍼파라미터로 학습된 모델을 사용하여 예측
best_svm  = grid_search.best_estimator_
y_pred = best_knn.predict(x_val)

# 정확도 평가
print(confusion_matrix(y_val, y_pred))
print('Classification Report  \n' , classification_report(y_val, y_pred ))


# 최적의 파라미터: {'C': 10 , 'kernel': 'rbf' , gammma : 0.1 }











# 파라미터 
param_grid = {
    #'C': [0.01, 0.1, 1, 10, 100], # 규제강도
    #'penalty': ['l1', 'l2', 'elasticnet', 'none'], # 규제 유형 라쏘 ,릿지 , 엘라스틱 , none 
    # 'solver': ['liblinear', 'saga', 'lbfgs', 'newton-cg'], # 최적화 알고리즘
    # 'max_iter': [100, 200, 500, 1000]  # 반복 횟수도 조정 가능
    'C' : [10],
    'penalty' :['l2'],
    'solver':['newton-cg']
    
}

model = LogisticRegression(max_iter = 100)

# GridSearchCV를 사용한 하이퍼파라미터 튜닝
grid_search = GridSearchCV(model, param_grid, cv=5, scoring='accuracy', n_jobs=1)
grid_search.fit(x_train, y_train) # scaling 된 데이터로 학습해도 되지만 성능 비교를 위해 scaling되지 않은 데이터 부터 학습 

# 최적의 하이퍼파라미터 출력
print(f'최적의 파라미터: {grid_search.best_params_}')

# 최적의 하이퍼파라미터로 학습된 모델을 사용하여 예측
best_log = grid_search.best_estimator_
y_pred = best_knn.predict(x_val)

# 정확도 평가
print(confusion_matrix(y_val, y_pred))
print('Classification Report  \n' , classification_report(y_val, y_pred ))



 # 최적의 파라미터 : {'C': 10, 'penalty': 'l2', 'solver': 'newton-cg'}











y_train.value_counts()


# 그라디언트 부스팅 XGB

xgb = XGBClassifier()

# 하이퍼파라미터 그리드 설정
param_grid = {
    #'n_estimators': [100, 200, 300],  # 트리 개수
    #'learning_rate': [0.01, 0.1, 0.3],  # 학습률
    #'max_depth': [3, 5, 7],  # 트리의 깊이
    #'subsample': [0.7, 0.8, 1.0],  # 데이터를 샘플링할 비율
    #'colsample_bytree': [0.7, 0.8, 1.0]  # 트리에서 사용할 특성 비율
    'n_estimators':[200],
    'max_depth':[3],
    'learning_rate': [0.3],
    'subsample': [0.8],
    'colsample_bytree': [1.0] 
    
}

# GridSearchCV를 사용한 하이퍼파라미터 튜닝
grid_search = GridSearchCV(xgb, param_grid, cv=5, scoring='accuracy', n_jobs=-1)
grid_search.fit(x_train, y_train) # 범주형(y train) 데이터를 0~5로 라벨링 

# 최적의 하이퍼파라미터 출력
print(f'Best Parameters: {grid_search.best_params_}')
# 최적의 하이퍼파라미터로 학습된 모델을 사용하여 예측
best_xgb = grid_search.best_estimator_
y_pred = best_xgb.predict(x_val)

# 정확도 평가
print(confusion_matrix(y_val, y_pred))
print('Classification Report  \n' , classification_report(y_val, y_pred ))


# Best Parameters: {'colsample_bytree': 0.8, 'learning_rate': 0.3, 'max_depth': 3, 'n_estimators': 200, 'subsample': 1.0}











# 사전 학습용 데이터 가변수화
#x_train.info() # x 는 전부 int() 연속형 데이터들이므로 가변수화를 하지 않아도 됩니다 
# 


best_lgbm = LGBMClassifier(n_estimators=260 ,learning_rate = 0.422,verbose =-1)# 최적의 파리미터는 이미 생성했습니다 
best_lgbm.fit(x_train,y_train)

y_pred = best_lgbm.predict(x_val)

# 정확도 평가(검증용)
print('============검증 결과==================')
print(confusion_matrix(y_val, y_pred))
print('Classification Report  \n' , classification_report(y_val, y_pred ))

y_pred = model_lgbm.predict(x_test)

# 정확도 평가(평가용)
print('============평가 결과==================')
print(confusion_matrix(y_test, y_pred))
print('Classification Report  \n' , classification_report(y_test, y_pred ))


# LightGBM을 하려면 컬럼명도 전부 바꿔줘야 해서 일단 skip
# lgbm = LGBMClassifier()

# # 하이퍼파라미터 그리드 설정
# param_grid = {
#     #'num_leaves': [31, 50, 100],  # 트리의 리프 개수
#     #'learning_rate': [0.01, 0.05, 0.1],  # 학습률
#     'n_estimators': [100, 200, 300],  # 트리 개수
#     #'max_depth': [-1, 5, 10],  # 트리의 최대 깊이 (-1은 제한 없음)
#     #'subsample': [0.7, 0.8, 1.0],  # 데이터를 샘플링할 비율
#     #'colsample_bytree': [0.7, 0.8, 1.0]  # 트리에서 사용할 특성 비율
# }

# # GridSearchCV를 사용한 하이퍼파라미터 튜닝
# grid_search = GridSearchCV(lgbm, param_grid, cv=5, scoring='accuracy', n_jobs=-1)
# grid_search.fit(x_train, y_train_encoded)

# # 최적의 하이퍼파라미터 출력
# print(f'Best Parameters: {grid_search.best_params_}')

# # 최적의 하이퍼파라미터로 학습된 모델을 사용하여 예측
# best_lgbm = grid_search.best_estimator_
# y_pred = best_lgbm.predict(x_val)

# # 정확도 평가
# print(confusion_matrix(y_val_encoded, y_pred))
# print('Classification Report  \n' , classification_report(y_val_encoded, y_pred ))














data_test.head()


# data_test에서 x_test와 y_test를 분할합니다.

target = 'Activity'
x_test = data_test.drop(columns=target)  
y_test = data_test.loc[:,target]
# (1)y_test 라벨링 
activity_mapping = {
    'LAYING': 0,
    'STANDING': 1,
    'SITTING': 2,
    'WALKING': 3,
    'WALKING_UPSTAIRS': 4,
    'WALKING_DOWNSTAIRS': 5
}
y_test = y_test.map(activity_mapping) # target label 0~5 변환

# (2) x_test 학습용 데이터도 minmax 스케일링을 해줬기 때문에 test도 같은 환경(minmax ) 스케일링 
# scaler.fit(x_train) 학습이 
x_test = scaler.transform(x_test)#  평가용 데이터 역시 fit을 해선 안됀다 


# 각 모델의 성능 평가
models = {
    "KNN": best_knn,
    "SVM": best_svm,
    "Logistic Regression": best_log,
    "XGBoost": best_xgb,
    "lightGBM": best_lgbm
}

for model_name, model in models.items():
    # 예측 수행
    y_pred = model.predict(x_test)
    
    # 성능 평가
    print(f"============={model_name} classification_report =============")
    print(classification_report(y_test, y_pred))
    
    # 혼동 행렬 출력
    cm = confusion_matrix(y_test, y_pred)
    print("V=====Confusion Matrix=====V")
    print(cm)
    print("\n")
































# SVM , Logistic Regress , XGBoost는 모든 파라미터를 탐색하지 못했으므로 최적의 파라미터를 찾지 못함
# 오늘 미니프로젝트 이후 다시 모델생성할 예정 








best_log = LogisticRegression(C=10, penalty = 'l2', solver= 'newton-cg' , max_iter = 100)
best_log.fit(x_train, y_train)
# 2. Support Vector Machine (서포트 벡터 머신)
best_svm = SVC(C=3, kernel='linear')
best_svm.fit(x_train, y_train)
# 3. K-Nearest Neighbors (KNN)
best_knn = KNeighborsClassifier(metric='manhattan', n_neighbors=3, weights='distance')
best_knn.fit(x_train, y_train)
# 4. LightGBM (Light Gradient Boosting Machine)
best_lgbm = LGBMClassifier(n_estimators=260, learning_rate=0.422)
best_lgbm.fit(x_train, y_train)


joblib.dump(best_svm, 'best_svm_model.pkl')
joblib.dump(best_log, 'best_log_model.pkl')
joblib.dump(best_knn, 'best_knn_model.pkl')
joblib.dump(best_lgbm, 'best_lgbm_model.pkl')



# 모델 불러오기 
model = joblib.load('best_lgbm_model.pkl')
y_pred = model.predict(x_test)

print('============평가 결과==================')
print(confusion_matrix(y_test, y_pred))
print('Classification Report  \n' , classification_report(y_test, y_pred ))



