























import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

import joblib

from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split





# 변수의 특성 중요도 계산하기
def plot_feature_importance(importance, names, result_only = False, topn = 'all' ,asd = False):
    feature_importance = np.array(importance)
    feature_name = np.array(names)

    data={'feature_name':feature_name,'feature_importance':feature_importance}
    fi_temp = pd.DataFrame(data)

    #변수의 특성 중요도 순으로 정렬하기
    fi_temp.sort_values(by=['feature_importance'], ascending=asd,inplace=True)
    fi_temp.reset_index(drop=True, inplace = True)

    if topn == 'all' :
        fi_df = fi_temp.copy()
    else :
        fi_df = fi_temp.iloc[:topn]

    #변수의 특성 중요도 그래프로 그리기
    if result_only == False :
        plt.figure(figsize=(10,20))
        sns.barplot(x='feature_importance', y='feature_name', data = fi_df)

        plt.xlabel('importance')
        plt.ylabel('feature name')
        plt.grid()

    return fi_df








data_train = pd.read_csv('data01_train.csv')


data_test = pd.read_csv('data01_test.csv')


drop_cols = 'subject'
data_test.drop(columns = drop_cols ,inplace =True)
data_train.drop(columns = drop_cols, inplace = True)











print(data_train.shape)
print(data_test.shape)


data_test.info() 


print(data_train.describe().T)


print(data_test.describe().T)








# Acrivity 범주의 종류 확인
categories_train = data_train['Activity'].unique()
categories_test = data_test['Activity'].unique()
print("train 데이터 범주의 종류: \n", categories_train)
print("test 데이터 범주의 종류: \n", categories_train)


# 범주별 빈도수 확인
activity_count_train = data_train['Activity'].value_counts()
activity_count_test = data_test['Activity'].value_counts()
print("train 데이터 범주별 빈도수: \n", activity_count_train)
print("test 데이터 범주별 빈도수: \n", activity_count_test)


# 범주별 비율 계산
activity_ratio_train = data_train['Activity'].value_counts(normalize=True)
activity_ratio_test = data_test['Activity'].value_counts(normalize=True)
print("train 데이터 범주별 비율(%) \n", activity_ratio_train)
print("test 데이터 범주별 비율(%) \n", activity_ratio_test)


# train , test 데이터 모두 같은 컬럼 명 , 우사한 비율로 구성되어있음 
# train 데이터에 한해서만 이변량 분석을 수행

















data_train.head()


#from sklearn.model_selection import train_test_split 이미 데이터가 분할되어 있으므로 쓰지 않음
target = 'Activity'
x_train = data_train.drop(columns= target)
x_test = data_test.drop(columns= target)
y_train= data_train.loc[:,target]
y_test= data_test.loc[:,target]






# 랜덤포레스트 '분류' 문제
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import *
model = RandomForestClassifier()

model.fit(x_train,y_train)

# 모델 성능 평가 (하지 않아도 됩니다)
y_pred = model.predict(x_test)

# 정확도 평가
print(confusion_matrix(y_test, y_pred))
print('Classification Report  \n' , classification_report(y_test, y_pred ))








'''
importance : 트리모델의 변수 중요도(예: model.feature_importances_)
names : 변수 이름 목록(예 : x_train.columns
result_only : 변수 중요도 순으로 데이터프레임만 return할지, 그래프도 포함할지 결정. False이면 결과 데이터프레임 + 그래프
topn : 중요도 상위 n개만 표시. all 이면 전체.
'''

tail5_feature=plot_feature_importance(model.feature_importances_, x_train.columns , result_only=True, topn=5,asd =True)
top5_feature=plot_feature_importance(model.feature_importances_, x_train.columns , result_only=True, topn=5,asd =False)














top5_feature
# 변수간 중요도는 모델 생성시 마다 바뀌기 때문에 반드시 이게 중요하다! 라고 할 순 없다 


tail5_feature





def plot_kde(features, df, target, title):
    for feature in features:
        plt.figure(figsize=(6, 4))
        # 타겟 변수의 각 범주에 대해 KDE plot을 그림
        for label in df[target].unique():
            sns.kdeplot(df[feature][df[target] == label], label=f'Target: {label}', fill=True,common_norm=True)
        plt.title(f'{title} - {feature}')
        plt.xlabel(feature)
        plt.ylabel('Density')
        plt.legend()
        plt.tight_layout()
        plt.show()


# 상위 5개 변수와 타겟 간의 KDE Plot
plot_kde(top5_feature['feature_name'], data_train, target, 'Top 5 Features')


# 상위 5개 변수들은 범주별로 확실히 구분되는 특징을 가짐 
# LAYING 누워있는거를 바로 구분 가능 





# 하위 5개 변수와 타겟 간의 KDE Plot
plot_kde(tail5_feature['feature_name'], data_train, target, 'Bottom 5 Features')


# 하위 5개 변수들은 밀도함수 분포가 곂쳐짐 > 범주별로 특성이 구분되지 않음 잘 분류할 수없음








target = 'Activity'  

feature = 'tGravityAcc-mean()-X'
plt.subplot(1,2,1)
sns.kdeplot(x=feature, data=data_train, hue=target , common_norm = True)
plt.grid()


feature = 'angle(X,gravityMean)'
plt.subplot(1,2,2)
sns.kdeplot(x=feature, data=data_train, hue=target , common_norm = True)
plt.grid()
plt.show()




















data_train['is_dynamic'] = data_train['Activity'].replace(
    {
        'STANDING': 0,
        'SITTING': 0,
        'LAYING': 0,
        'WALKING': 1,
        'WALKING_UPSTAIRS': 1,
        'WALKING_DOWNSTAIRS': 1
        
    }
)

data_train.drop(columns = 'Activity',inplace= True)
#print(data_train.head())

data_test['is_dynamic'] = data_test['Activity'].replace(
    {
        'STANDING': 0,
        'SITTING': 0,
        'LAYING': 0,
        'WALKING': 1,
        'WALKING_UPSTAIRS': 1,
        'WALKING_DOWNSTAIRS': 1
        
    }
)

data_test.drop(columns = 'Activity',inplace= True)
#print(data_test.head())





# 타겟 재지정 
target = 'is_dynamic'

x_train = data_train.drop(columns= target)
x_test = data_test.drop(columns= target)
y_train= data_train.loc[:,target]
y_test= data_test.loc[:,target]


#from sklearn.ensemble import RandomForestClassifier
model = RandomForestClassifier()
model.fit(x_train,y_train)
# 성능평가
y_pred = model.predict(x_test)

print(confusion_matrix(y_test, y_pred))
print('Classification Report \n' , classification_report(y_test, y_pred ))


# 너무 정확도가 높게 나왔는데??





top5_feature=plot_feature_importance(model.feature_importances_, x_train.columns , result_only=True, topn=5)
top5_feature


def plot_kde_2(features, df, target, title):
    for feature in features:
        plt.figure(figsize=(15, 4))
        # 타겟 변수의 각 범주에 대해 KDE plot을 그림
        plt.subplot(1,3,1)
        sns.kdeplot(data=df,x=feature, hue =target,common_norm=True )
        plt.xlabel(feature)
        plt.legend([f'{target}:0 ',f'{target}:1 '])

        plt.subplot(1,3,2)
        sns.kdeplot(data=df,x=feature, hue =target ,multiple ='fill' ,common_norm=True)
        plt.title(f'{title} - {feature}')
        plt.xlabel(feature)
        plt.legend([f'{target}:0 ',f'{target}:1 '])
        
        plt.subplot(1,3,3)
        sns.boxplot(data=df,x=feature, hue =target )
        plt.xlabel(feature)
        plt.legend([f'{target}:0 ',f'{target}:1 '])
        
        plt.show()


def plot_boxplot(features, df, target, title):
    for feature in features:
        plt.figure(figsize=(15, 4))
        # 타겟 변수의 각 범주에 대해 boxplot을 그림
        sns.boxplot(data=df,x=feature, hue =target )
        plt.title(f'{title} - {feature}')
        plt.xlabel(feature)
        plt.legend([f'{target}:0 ',f'{target}:1 '])
        
        plt.show()


# 상위 5개 변수와 타겟 간의 KDE Plot
plot_kde_2(top5_feature['feature_name'], data_train, target, 'Top 5 Features')


plot_boxplot(top5_feature['feature_name'], data_train, target, 'Top 5 Features')


bottom5_feature=plot_feature_importance(model.feature_importances_, x_train.columns , result_only=True, topn=5 ,asd= True)
bottom5_feature


# 하위 5개 변수와 타겟 간의 KDE Plot
plot_kde_2(bottom5_feature['feature_name'], data_train, target, 'Bottom 5 Features')


plot_boxplot(bottom5_feature['feature_name'], data_train, target, 'Bottom 5 Features')








top5_feature


target = 'is_dynamic'  

feature = 'fBodyAccJerk-bandsEnergy()-1,16'
plt.subplot(1,2,1)
sns.kdeplot(x=feature, data=data_train, hue=target , common_norm = True)
plt.grid()



feature = 'fBodyAccJerk-entropy()-X'
plt.subplot(1,2,2)
sns.kdeplot(x=feature, data=data_train, hue=target , common_norm = True)
plt.grid()
plt.show()









