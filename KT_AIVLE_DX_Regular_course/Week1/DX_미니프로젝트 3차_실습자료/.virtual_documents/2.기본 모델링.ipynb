

















import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from tqdm.auto import tqdm
import warnings
warnings.filterwarnings('ignore')

import joblib

# 필요한 라이브러리, 함수 로딩 ------------------
from sklearn.tree import DecisionTreeClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.linear_model import LogisticRegression
from skelearn.svm import SVR
from sklearn.model_selection import GridSearchCV
from sklearn.preprocessing import MinMAxScaler

# 하이퍼 파라미터 튜닝은 _val로 진행
# 모델 성능 비교는 data_test로 진행
# 변수를 삭제?  > 타당한 근거가 있어야 함  ex) Tree계열 알고리즘은 변수가 많아도 안정적인 성능
#                                               LGBM ,로지스틱 회귀 , KNN은 변수가 많아지거나 이상한 변수가 추가되면 성능하락
#                                               옵션(주성분 분석)






# 변수의 특성 중요도 계산하기
def plot_feature_importance(importance, names, result_only = False, topn = 'all'):
    feature_importance = np.array(importance)
    feature_name = np.array(names)
    
    data={'feature_name':feature_name,'feature_importance':feature_importance}
    fi_temp = pd.DataFrame(data)

    #변수의 특성 중요도 순으로 정렬하기
    fi_temp.sort_values(by=['feature_importance'], ascending=False,inplace=True)
    fi_temp.reset_index(drop=True, inplace = True)

    if topn == 'all' :
        fi_df = fi_temp.copy()
    else :
        fi_df = fi_temp.iloc[:topn]

    #변수의 특성 중요도 그래프로 그리기
    if result_only == False :
        plt.figure(figsize=(10,20))
        sns.barplot(x='feature_importance', y='feature_name', data = fi_df)

        plt.xlabel('importance')
        plt.ylabel('feature name')
        plt.grid()

    return fi_df











data_train = pd.read_csv('data01_train.csv')


data_test = pd.read_csv('data01_test.csv')


drop_cols = 'subject'
data_test.drop(columns = drop_cols ,inplace =True)
data_train.drop(columns = drop_cols, inplace = True)





print(data_train.shape)
print(data_test.shape)


data_test.info() 


# Acrivity 범주의 종류 확인
categories_train = data_train['Activity'].unique()
categories_test = data_test['Activity'].unique()
print("train 데이터 범주의 종류: \n", categories_train)
print("test 데이터 범주의 종류: \n", categories_train)


# 범주별 빈도수 확인
activity_count_train = data_train['Activity'].value_counts()
activity_count_test = data_test['Activity'].value_counts()
print("train 데이터 범주별 빈도수: \n", activity_count_train)
print("test 데이터 범주별 빈도수: \n", activity_count_test)


# 범주별 비율 계산
activity_ratio_train = data_train['Activity'].value_counts(normalize=True)
activity_ratio_test = data_test['Activity'].value_counts(normalize=True)
print("train 데이터 범주별 비율(%) \n", activity_ratio_train)
print("test 데이터 범주별 비율(%) \n", activity_ratio_test)











target = 'Activity'
x = data_train.drop(columns= target)
y = data_train.loc[:,target]









from sklearn.model_selection import train_test_split

x_train , x_test , y_train , y_test = train_test_split(x,y,random_state=42,test_size=0.2)








from sklearn.preprocessing import MinMaxScaler
scaler = MinMaxScaler()
x_train_s = scaler.fit_transform(x_train)
x_test_s = scaler.transform(x_test)












# 로지스틱 회귀














# 랜덤포레스트














# 서포트벡터머신














# 그라디언트 부스팅














# Knn근접이웃법

































