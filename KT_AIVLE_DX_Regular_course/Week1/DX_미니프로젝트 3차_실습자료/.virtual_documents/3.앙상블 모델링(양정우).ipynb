

















import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from tqdm.auto import tqdm
import warnings
warnings.filterwarnings('ignore')

import joblib
from sklearn.ensemble import StackingClassifier, VotingClassifier

# 필요한 라이브러리, 함수 로딩 ------------------
from sklearn.model_selection import train_test_split
from sklearn.metrics import *
from sklearn.preprocessing import MinMaxScaler
from sklearn.linear_model import LogisticRegression






# 변수의 특성 중요도 계산하기
def plot_feature_importance(importance, names, result_only = False, topn = 'all'):
    feature_importance = np.array(importance)
    feature_name = np.array(names)

    data={'feature_name':feature_name,'feature_importance':feature_importance}
    fi_temp = pd.DataFrame(data)

    #변수의 특성 중요도 순으로 정렬하기
    fi_temp.sort_values(by=['feature_importance'], ascending=False,inplace=True)
    fi_temp.reset_index(drop=True, inplace = True)

    if topn == 'all' :
        fi_df = fi_temp.copy()
    else :
        fi_df = fi_temp.iloc[:topn]

    #변수의 특성 중요도 그래프로 그리기
    if result_only == False :
        plt.figure(figsize=(10,20))
        sns.barplot(x='feature_importance', y='feature_name', data = fi_df)

        plt.xlabel('importance')
        plt.ylabel('feature name')
        plt.grid()

    return fi_df











file1 = 'data01_train.csv'
file2 = 'data01_test.csv'


data = pd.read_csv(file1)
test = pd.read_csv(file2)


# 불필요한 칼럼 삭제
data.drop('subject', axis=1, inplace=True)
test.drop('subject', axis=1, inplace=True)





#전체 데이터의 행,열 개수 확인
data.shape


#전체 데이터의 상위 5개 행 확인
data.head()


#전체 데이터의 수치형 변수 분포 확인
data.describe()


#전체 데이터의 모든 변수 확인
data.columns











drop_cols ='Activity'
x = data.drop(columns = drop_cols)
y = data.loc[:,drop_cols]

x_test = test.drop(columns = drop_cols)
y_test= test.loc[:,drop_cols]








x_train , x_val , y_train , y_val = train_test_split(x,y,test_size = 0.3)








scaler = MinMaxScaler()
scaler.fit(x_train)
x_train = scaler.transform(x_train)
x_val = scaler.transform(x_val)
x_test = scaler.transform(x_test)





x_train = np.ascontiguousarray(x_train)
x_val = np.ascontiguousarray(x_val)





#방법 2  대체 
activity_mapping = {
    'LAYING': 0,
    'STANDING': 1,
    'SITTING': 2,
    'WALKING': 3,
    'WALKING_UPSTAIRS': 4,
    'WALKING_DOWNSTAIRS': 5
}
y_train = y_train.map(activity_mapping)
y_val = y_val.map(activity_mapping)
y_test = y_test.map(activity_mapping)











model_knn = joblib.load('best_knn_model.pkl')
model_svm = joblib.load('best_svm_model.pkl')
model_log = joblib.load('best_log_model.pkl')
model_lgbm = joblib.load('best_lgbm_model.pkl')





# 스태킹 앙상블 모델 정의
base_models = [
    ('knn', model_knn),
    ('svm', model_svm),
    ('logistic', model_log),
    ('lgbm', model_lgbm)
]


from sklearn.ensemble import VotingClassifier
# 방법 1 : 하드보팅(다수) 
voting_model = VotingClassifier(estimators=base_models ,voting= 'hard')
# 보팅 모델 학습 
voting_model.fit(x_train , y_train)
# 성능 확인 
y_pred = voting_model.predict(x_test)

# 성능 확인  
print('============평가 결과==================')
print(confusion_matrix(y_test, y_pred))
print('============Classification Report============ \n' , classification_report(y_test, y_pred ))


# 보팅 2 소프트보팅 (SVM 모델은 sofe 보팅이 불가능함 )
#
# voting_model = VotingClassifier(estimators=base_models ,voting= 'soft')
# # 보팅 모델 학습 
# voting_model.fit(x_train , y_train)
# # 성능 확인 
# y_pred = voting_model.predict(x_test)

# # 성능 확인  
# print('============평가 결과==================')
# print(confusion_matrix(y_test, y_pred))
# print('============Classification Report============ \n' , classification_report(y_test, y_pred ))

# 이찬울님의 의견 (svm 모델을 앙상블에서 제외하고 대신 knn 새로운 모델하나 추가)
from sklearn.neighbors import KNeighborsClassifier

model_knn2 = KNeighborsClassifier(metric= 'manhattan', n_neighbors= 12, weights= 'distance')# 최적의 파라미터는 GridSearch로 찾았습니다
model_knn2.fit(x_train,y_train)

base_models = [
    ('knn', model_knn),
    ('knn2', model_knn2),# svm 변경<< svm 모델이였는데 변경 
    ('logistic', model_log),
    ('lgbm', model_lgbm)
]

voting_model = VotingClassifier(estimators=base_models ,voting= 'soft')
# 보팅 모델 학습 
voting_model.fit(x_train , y_train)
# 성능 확인 
y_pred = voting_model.predict(x_test)

# 성능 확인  
print('============평가 결과==================')
print(confusion_matrix(y_test, y_pred))
print('============Classification Report============ \n' , classification_report(y_test, y_pred ))








from sklearn.ensemble import StackingClassifier #

# 4개 모델을을 최종으로 학습할 메타 모델은 로지스틱 회귀 모델로 선정 
meta_model = LogisticRegression()

stacked_model = StackingClassifier(estimators=base_models, final_estimator=meta_model)

# 모델 학습
stacked_model.fit(x_train, y_train)
# 예측 
y_pred = stacked_model.predict(x_test)
# 성능 확인  
print('============평가 결과==================')
print(confusion_matrix(y_test, y_pred))
print('============Classification Report============ \n' , classification_report(y_test, y_pred ))












# new 데이터가 들어왔을때  
# input data =  .csv 파일형태임 
# 1. 데이터 전처리
# 2. 예측 
# 3. 평가

# output : 모델평가 , 예측결과 
# 
def pipeline(x_train,input_file, del_columns , y_label, ensamble_model):
    '''
        주의사항!
        1. input file은 .csv 형태
        2. 
    
    '''
    # 1. 데이터 전처리 부분 
    test = pd.read_csv(input_file)
    test.drop(del_columns, axis=1, inplace=True)
    x_test = test.drop(columns = y_label)
    y_test= test.loc[:,y_label]
    # 스케일링 
    scaler = MinMaxScaler()
    scaler.fit(x_train)
    x_test = scaler.transform(x_test)
    print(x_test)
    # Activity 라벨 변경 
    activity_mapping = {
        'LAYING': 0,
        'STANDING': 1,
        'SITTING': 2,
        'WALKING': 3,
        'WALKING_UPSTAIRS': 4,
        'WALKING_DOWNSTAIRS': 5
    }
    y_test = y_test.replace(activity_mapping)
    print(y_test)
    # 2. 스태킹 데이터로 
    y_pred = ensamble_model.predict(x_test)
    # 3. 평가지표 반환

    return (confusion_matrix(y_test, y_pred) , classification_report(y_test, y_pred))
        
    
    


file2 = 'data01_test.csv'
drop_cols = ['subject']
y_label = ['Activity']
A , B = pipeline(x_train=x_train ,input_file=file2 , del_columns=drop_cols , y_label=y_label ,
                ensamble_model= stacked_model)

print(A)
print(B)









